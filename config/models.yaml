# Backend configuration
defaults:
  backends:
    # https://github.com/ggerganov/llama.cpp/releases
    # Staying on b7531 (last version before that change) for stable performance
    llama:
      version: b7531
      image_type: build
      args:
        n_gpu_layers: 999

# Models configuration
models:
  - repo_id: unsloth/qwen2.5
    include:
      - "qwen2.5/*.gguf"
    backends: llama

# Benchmark configuration
benchmark_questions:
  hard_questions:
    - prompt: "Analyze the philosophical implications of consciousness transfer in AI systems. How would uploading human consciousness to a digital substrate affect personal identity, continuity of self, and the nature of existence? Consider both materialist and dualist perspectives, and address the hard problem of consciousness in your analysis."
      max_tokens: 300
      description: "Complex philosophical reasoning and consciousness theory"

    - prompt: "Design a comprehensive solution for a distributed quantum computing network that can handle fault-tolerant quantum error correction across multiple geographic locations. Address the challenges of quantum decoherence, entanglement distribution, classical-quantum interfaces, and the economic implications of such a system. Include specific protocols and algorithms."
      max_tokens: 350
      description: "Advanced technical problem-solving in quantum computing"

    - prompt: "Construct a detailed mathematical proof that demonstrates why P vs NP is such a fundamental question in computational complexity theory. Then, propose a novel approach to tackling this problem by examining the relationship between circuit complexity, proof complexity, and algebraic geometry. Support your approach with rigorous mathematical reasoning."
      max_tokens: 400
      description: "Advanced mathematics and theoretical computer science"

quick_benchmark:
  prompt_set: quick
  prompt: "Write a short Python function to find the maximum number in a list."
  max_tokens: 50
  temperature: 0.7
  iterations: 2

standard_benchmark:
  prompt_set: standard
  prompt: "Explain quantum computing in simple terms and provide a real-world analogy."
  max_tokens: 100
  temperature: 0.7
  iterations: 3

performance_benchmark:
  prompt_set: performance
  prompt: "Write a comprehensive guide on machine learning algorithms, including examples and use cases for each type."
  max_tokens: 200
  temperature: 0.7
  iterations: 5
